{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo Práctico 1 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiploDatos 2018 - Aprendizaje No Supervizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mario Ferreyra - Emiliano Kokic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos: Dataset of references (urls) to news web pages\n",
    "https://archive.ics.uci.edu/ml/datasets/News+Aggregator\n",
    "\n",
    "Descripción del problema: el dataset contiene dentro de sus atributos título y categoria de noticia:\n",
    "- Entretenimiento\n",
    "- Ciencia y Tecnología\n",
    "- Negocios\n",
    "- Salud\n",
    "\n",
    "La idea del problema sería intentar aplicar técnicas de clustering sobre los títulos de cada noticia y ver si de acuerdo a su semántica se agrupan según las distintas categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nID\\t\\tNumeric ID\\nTITLE\\t\\tNews title \\nURL\\t\\tUrl\\nPUBLISHER\\tPublisher name\\nCATEGORY\\tNews category (b = business, t = science and technology, e = entertainment, m = health)\\nSTORY\\t\\tAlphanumeric ID of the cluster that includes news about the same story\\nHOSTNAME\\tUrl hostname\\nTIMESTAMP \\tApproximate time the news was published, as the number of milliseconds since the epoch\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./NewsAggregatorDataset/newsCorpora.csv', header=None, delimiter='\\t')\n",
    "data.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "'''\n",
    "ID\t\tNumeric ID\n",
    "TITLE\t\tNews title \n",
    "URL\t\tUrl\n",
    "PUBLISHER\tPublisher name\n",
    "CATEGORY\tNews category (b = business, t = science and technology, e = entertainment, m = health)\n",
    "STORY\t\tAlphanumeric ID of the cluster that includes news about the same story\n",
    "HOSTNAME\tUrl hostname\n",
    "TIMESTAMP \tApproximate time the news was published, as the number of milliseconds since the epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  Fed official says weak data caused by weather,...        b\n",
       "1  Fed's Charles Plosser sees high bar for change...        b\n",
       "2  US open: Stocks fall after Fed official hints ...        b\n",
       "3  Fed risks falling 'behind the curve', Charles ...        b\n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos quedamos solo con las dos columnas de interés\n",
    "data = data[['TITLE', 'CATEGORY']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITLE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The article requested cannot be found! Please refresh your browser or go back  ...</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Highlights</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Posted by Parvez Jabri</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Posted by Imaduddin</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Posted by Shoaib-ur-Rehman Siddiqui</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    CATEGORY\n",
       "TITLE                                                       \n",
       "The article requested cannot be found! Please r...       145\n",
       "Business Highlights                                       59\n",
       "Posted by Parvez Jabri                                    59\n",
       "Posted by Imaduddin                                       53\n",
       "Posted by Shoaib-ur-Rehman Siddiqui                       52"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('TITLE').count().sort_values('CATEGORY',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos las filas sin título ya que no aportan información y a la hora de realizar clustering van a generar ruido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article requested cannot be found! Please refresh your browser or go back  ...\n"
     ]
    }
   ],
   "source": [
    "no_title = data.groupby('TITLE').count().sort_values('CATEGORY',ascending=False).head().index.values[0]\n",
    "print(no_title)\n",
    "data = data[data.TITLE != no_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>115965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>152339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>45637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>108333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TITLE\n",
       "CATEGORY        \n",
       "b         115965\n",
       "e         152339\n",
       "m          45637\n",
       "t         108333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('CATEGORY').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con 2000 títulos de cada categoría (ya que sino se necesita demasiado cómputo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TITLE\n",
       "CATEGORY       \n",
       "b          2000\n",
       "e          2000\n",
       "m          2000\n",
       "t          2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data[data.CATEGORY == 'b'][0:2000]\n",
    "new_data = new_data.append(data[data.CATEGORY == 'e'][0:2000], ignore_index=True)\n",
    "new_data = new_data.append(data[data.CATEGORY == 'm'][0:2000], ignore_index=True)\n",
    "new_data = new_data.append(data[data.CATEGORY == 't'][0:2000], ignore_index=True)\n",
    "new_data.groupby('CATEGORY').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es entonces aplicar clustering sobre los títulos de las noticias y ver si se agrupan por categoría de acuerdo a su semantica. Es decir, vamos a considerar 4 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del texto | tokenizing\n",
    "\n",
    "Para analizar el texto debemos estudiar la frecuencia de las palabras, es decir, separar el texto en unidades sintácticas o *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_tokenized = []\n",
    "\n",
    "for index, row in new_data.iterrows():\n",
    "    allwords_tokenized = tokenize_only(row['TITLE'])\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay en total 71705 tokens \n",
      "\n",
      "['fed', 'official', 'says', 'weak', 'data', 'caused', 'by', 'weather', 'should', 'not', 'slow', 'taper', 'fed', \"'s\", 'charles', 'plosser', 'sees', 'high', 'bar', 'for', 'change', 'in', 'pace', 'of', 'tapering', 'us', 'open', 'stocks', 'fall', 'after', 'fed', 'official', 'hints', 'at', 'accelerated', 'tapering', 'fed', 'risks', 'falling', \"'behind\", 'the', 'curve', 'charles', 'plosser', 'says', 'fed', \"'s\", 'plosser', 'nasty', 'weather']\n"
     ]
    }
   ],
   "source": [
    "print('Hay en total ' + str(len(totalvocab_tokenized)) + ' tokens \\n')\n",
    "len(totalvocab_tokenized)\n",
    "print (totalvocab_tokenized[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 62858)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_only, \n",
    "                                   ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(new_data['TITLE'])\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar clusters | Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 694 elementos\n",
      "El cluster 1 tiene 6890 elementos\n",
      "El cluster 2 tiene 158 elementos\n",
      "El cluster 3 tiene 258 elementos\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# print (clusters)\n",
    "\n",
    "# Recuento del número de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          title\n",
      "category       \n",
      "e           622\n",
      "m            72\n",
      "          title\n",
      "category       \n",
      "b          2000\n",
      "e          1378\n",
      "m          1512\n",
      "t          2000\n",
      "          title\n",
      "category       \n",
      "m           158\n",
      "          title\n",
      "category       \n",
      "m           258\n"
     ]
    }
   ],
   "source": [
    "news = { 'title': new_data['TITLE'].tolist(), 'category': new_data['CATEGORY'].tolist(), 'cluster': clusters}\n",
    "news = pd.DataFrame(news, index = [clusters] , columns = ['title', 'category'])\n",
    "print(news.loc[0].groupby('category').count())\n",
    "print(news.loc[1].groupby('category').count())\n",
    "print(news.loc[2].groupby('category').count())\n",
    "print(news.loc[3].groupby('category').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que uno de los clusters concentra casi la totalidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probemos utilizando el tokenizer de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTA: token.is_stop nos devuelve un booleano que indica si el token es o no una stopword\n",
    "nlp = spacy.load('en')\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)\n",
    "tokens = []\n",
    "for doc in tokenizer.pipe(new_data['TITLE']):\n",
    "    tokens.append([token.text for token in doc if re.search('[a-zA-Z]', token.text)\n",
    "                   and not token.is_stop])    \n",
    "# tokens es una lista de listas donde cada lista contiene los tokens de cada titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fed',\n",
       " 'official',\n",
       " 'says',\n",
       " 'weak',\n",
       " 'data',\n",
       " 'caused',\n",
       " 'weather',\n",
       " 'slow',\n",
       " 'taper']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probemos utilizando gensim para generar word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = tokens\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10006"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilicemos KMeans de nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance)\n",
    "assigned_clusters = kclusterer.cluster(word_vectors.vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 7406 elementos\n",
      "El cluster 1 tiene 75 elementos\n",
      "El cluster 2 tiene 1336 elementos\n",
      "El cluster 3 tiene 1189 elementos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10006"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(NUM_CLUSTERS):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, assigned_clusters.count(i)))\n",
    "len(assigned_clusters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed official says weak data caused by weather, should not slow taper\n",
      "Fed's Charles Plosser sees high bar for change in pace of tapering\n",
      "0.0644\n"
     ]
    }
   ],
   "source": [
    "# >> sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "# >>> sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "# >>> similarity = word_vectors.wmdistance(sentence_obama, sentence_president)\n",
    "# >>> print(\"{:.4f}\".format(similarity))\n",
    "print(new_data['TITLE'][0])\n",
    "print(new_data['TITLE'][1])\n",
    "similarity = word_vectors.wmdistance(new_data['TITLE'][0], new_data['TITLE'][1])\n",
    "print(\"{:.4f}\".format(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'The',\n",
       " 'Titanfall',\n",
       " 'To',\n",
       " 'Bachelor',\n",
       " 'US',\n",
       " 'New',\n",
       " 'Xbox',\n",
       " 'SXSW',\n",
       " 'One',\n",
       " 'In',\n",
       " 'Juan',\n",
       " 'Pablo',\n",
       " 'Snowden',\n",
       " 'For',\n",
       " 'A',\n",
       " 'Google',\n",
       " 'Bieber',\n",
       " 'Is',\n",
       " 'Alzheimer',\n",
       " 'Of',\n",
       " 'Justin',\n",
       " 'test',\n",
       " 'China',\n",
       " 'With',\n",
       " 'Lena',\n",
       " 'Dunham',\n",
       " 'GM',\n",
       " 'cancer',\n",
       " 'On',\n",
       " 'True',\n",
       " 'iOS',\n",
       " 'says',\n",
       " 'new',\n",
       " 'Apple',\n",
       " 'Detective',\n",
       " 'Cancer',\n",
       " 'And',\n",
       " 'Season',\n",
       " 'Game',\n",
       " 'Study',\n",
       " \"n't\",\n",
       " 'Lindsay',\n",
       " 'Blood',\n",
       " 'Cosmos',\n",
       " 'Miley',\n",
       " 'Neil',\n",
       " 'Colorado',\n",
       " 'Gox',\n",
       " 'Cyrus',\n",
       " 'FDA',\n",
       " 'Will',\n",
       " 'Thrones',\n",
       " 'Edward',\n",
       " 'Keibler',\n",
       " 'Stacy',\n",
       " 'Finale',\n",
       " 'Lohan',\n",
       " 'Live',\n",
       " 'After',\n",
       " 'Chiquita',\n",
       " 'Mt.',\n",
       " 'Young',\n",
       " 'recall',\n",
       " 'Carney',\n",
       " 'Bank',\n",
       " 'Microsoft',\n",
       " 'heart',\n",
       " 'May',\n",
       " 'You',\n",
       " 'Stocks',\n",
       " 'More',\n",
       " 'What',\n",
       " 'Bitcoin',\n",
       " 'risk',\n",
       " 'NSA',\n",
       " 'It',\n",
       " 'Selena',\n",
       " 'Drug',\n",
       " 'Android',\n",
       " 'Gomez',\n",
       " 'TV',\n",
       " 'study',\n",
       " 'data',\n",
       " 'sales',\n",
       " 'George',\n",
       " 'Colon',\n",
       " 'Flappy',\n",
       " 'Company',\n",
       " 'Twitter',\n",
       " 'Mobile',\n",
       " 'Up',\n",
       " 'CEO',\n",
       " 'American',\n",
       " 'High',\n",
       " 'Fyffes',\n",
       " 'Galavis',\n",
       " 'Ukraine',\n",
       " 'bankruptcy',\n",
       " 'Be',\n",
       " 'files',\n",
       " 'I',\n",
       " 'Bird',\n",
       " 'Test',\n",
       " 'blood',\n",
       " 'First',\n",
       " 'Swift',\n",
       " 'Video',\n",
       " 'health',\n",
       " 'Watch',\n",
       " 'Taylor',\n",
       " 'drug',\n",
       " 'T',\n",
       " 'Says',\n",
       " 'Not',\n",
       " 'From',\n",
       " 'finale',\n",
       " 'Why',\n",
       " 'How',\n",
       " 'Review',\n",
       " 'SNL',\n",
       " 'Update',\n",
       " 'Can',\n",
       " 'marijuana',\n",
       " 'HIV',\n",
       " 'Risk',\n",
       " 'Comcast',\n",
       " 'Bossy',\n",
       " 'stocks',\n",
       " 'About',\n",
       " 'Recall',\n",
       " 'iPhone',\n",
       " 'disease',\n",
       " 'company',\n",
       " 'No',\n",
       " 'Sprint',\n",
       " 'Fannie',\n",
       " 'Girls',\n",
       " 'Zac',\n",
       " 'Efron',\n",
       " 'Heart',\n",
       " 'By',\n",
       " 'million',\n",
       " 'Sbarro',\n",
       " 'Gas',\n",
       " 'Could',\n",
       " 'oil',\n",
       " 'Outfitters',\n",
       " 'Who',\n",
       " 'Trailer',\n",
       " 'Release',\n",
       " 'World',\n",
       " 'HBO',\n",
       " 'Health',\n",
       " 'Time',\n",
       " 'gas',\n",
       " 'House',\n",
       " 'VIDEO',\n",
       " 'Voice',\n",
       " 'U.S.',\n",
       " 'Launch',\n",
       " 'Ban',\n",
       " 'March',\n",
       " 'Jared',\n",
       " 'Harry',\n",
       " 'SDK',\n",
       " 'prices',\n",
       " 'At',\n",
       " 'We',\n",
       " 'video',\n",
       " 'Clooney',\n",
       " 'predict',\n",
       " 'Report',\n",
       " 'Freddie',\n",
       " 'Show',\n",
       " 'Her',\n",
       " 'Nikki',\n",
       " 'ADHD',\n",
       " 'This',\n",
       " 'Are',\n",
       " 'lower',\n",
       " 'Do',\n",
       " 'Marijuana',\n",
       " 'gets',\n",
       " 'Pobre',\n",
       " 'banana',\n",
       " 'Prince',\n",
       " 'Menino',\n",
       " 'Get',\n",
       " 'prevent',\n",
       " '3D',\n",
       " 'Chocolate',\n",
       " 'Gets',\n",
       " 'Now',\n",
       " 'Recap',\n",
       " 'Down',\n",
       " 'News',\n",
       " 'Spoilers',\n",
       " 'taxes',\n",
       " 'launch',\n",
       " 'Men',\n",
       " 'chocolate',\n",
       " 'market',\n",
       " 'pot',\n",
       " 'Night',\n",
       " 'Beyonce',\n",
       " 'CarPlay',\n",
       " 'As',\n",
       " 'Americans',\n",
       " 'Chinese',\n",
       " 'Japan',\n",
       " 'Your',\n",
       " 'Urban',\n",
       " 'England',\n",
       " 'weak',\n",
       " 'Top',\n",
       " 'Sales',\n",
       " 'face',\n",
       " 'UK',\n",
       " 'month',\n",
       " 'game',\n",
       " 'Use',\n",
       " 'approves',\n",
       " 'Does',\n",
       " 'news',\n",
       " 'public',\n",
       " 'season',\n",
       " 'time',\n",
       " 'colon',\n",
       " 'update',\n",
       " 'Apps',\n",
       " 'makes',\n",
       " 'South',\n",
       " 'Libya',\n",
       " 'music',\n",
       " 'Music',\n",
       " 'Favreau',\n",
       " 'Disease',\n",
       " 'UPDATE',\n",
       " 'help',\n",
       " 'Here',\n",
       " 'Money',\n",
       " 'Public',\n",
       " 'January',\n",
       " 'Wearhouse',\n",
       " 'Jos',\n",
       " 'PC',\n",
       " 'Tyson',\n",
       " 'Has',\n",
       " 'deal',\n",
       " 'price',\n",
       " 'live',\n",
       " 'exchange',\n",
       " 'Former',\n",
       " 'use',\n",
       " 'Price',\n",
       " 'world',\n",
       " 'Inc.',\n",
       " 'Libyan',\n",
       " 'sign',\n",
       " 'probe',\n",
       " 'Was',\n",
       " 'Saturday',\n",
       " 'Jon',\n",
       " 'Clare',\n",
       " 'Have',\n",
       " 'That',\n",
       " 'Zimmerman',\n",
       " 'trailer',\n",
       " 'life',\n",
       " 'Market',\n",
       " 'Out',\n",
       " 'M',\n",
       " 'Merger',\n",
       " 'Medical',\n",
       " 'Josh',\n",
       " 'Sandberg',\n",
       " 'linked',\n",
       " 'Twitch',\n",
       " 'SoftBank',\n",
       " 'rise',\n",
       " 'Business',\n",
       " 'years',\n",
       " 'Asia',\n",
       " 'North',\n",
       " 'Month',\n",
       " 'Eagle',\n",
       " 'Boy',\n",
       " 'Device',\n",
       " 'cell',\n",
       " 'migraine',\n",
       " 'tech',\n",
       " 'early',\n",
       " 'Over',\n",
       " 'weeks',\n",
       " 'BoE',\n",
       " 'Day',\n",
       " 'School',\n",
       " 'Ferrell',\n",
       " 'Son',\n",
       " 'device',\n",
       " 'Cressida',\n",
       " 'boy',\n",
       " 'Respawn',\n",
       " 'His',\n",
       " 'Did',\n",
       " 'goes',\n",
       " 'General',\n",
       " 'Dick',\n",
       " 'Sporting',\n",
       " 'A.',\n",
       " 'Fitbit',\n",
       " 'McCarthy',\n",
       " 'high',\n",
       " 'McDonald',\n",
       " 'But',\n",
       " 'Bankruptcy',\n",
       " 'Prevent',\n",
       " 'Goods',\n",
       " 'rebel',\n",
       " 'He',\n",
       " 'advanced',\n",
       " 'ex',\n",
       " 'Chef',\n",
       " 'release',\n",
       " 'Auditions',\n",
       " 'Blind',\n",
       " 'headband',\n",
       " 'Hardy',\n",
       " 'wearable',\n",
       " 'Jenny',\n",
       " 'Autism',\n",
       " 'wearables',\n",
       " 'App',\n",
       " 'open',\n",
       " 'Mark',\n",
       " 'Files',\n",
       " 'Airlines',\n",
       " 'Parents',\n",
       " 'old',\n",
       " 'Facebook',\n",
       " 'care',\n",
       " 'Mexico',\n",
       " 'Women',\n",
       " 'deposition',\n",
       " 'marries',\n",
       " 'Sheryl',\n",
       " 'Researchers',\n",
       " 'pills',\n",
       " 'Again',\n",
       " 'record',\n",
       " 'Asian',\n",
       " 'Back',\n",
       " 'Big',\n",
       " 'Oil',\n",
       " 'Banana',\n",
       " 'Penney',\n",
       " 'Netflix',\n",
       " 'Episode',\n",
       " 'Final',\n",
       " 'ups',\n",
       " 'Creator',\n",
       " 'bossy',\n",
       " 'experimental',\n",
       " 'Colorectal',\n",
       " 'Wall',\n",
       " 'Man',\n",
       " 'Neighbors',\n",
       " 'Even',\n",
       " 'shows',\n",
       " 'campaign',\n",
       " 'Series',\n",
       " 'Star',\n",
       " 'Musical',\n",
       " 'Billboard',\n",
       " 'migraines',\n",
       " '7-year',\n",
       " '792p',\n",
       " 'Motors',\n",
       " 'growth',\n",
       " 'hit',\n",
       " 'Data',\n",
       " 'Icahn',\n",
       " 'Stock',\n",
       " 'All',\n",
       " 'Old',\n",
       " 'Still',\n",
       " 'Go',\n",
       " 'Hackers',\n",
       " 'attacks',\n",
       " 'Metro',\n",
       " 'Russia',\n",
       " 'Help',\n",
       " 'dying',\n",
       " 'Life',\n",
       " 'Deposition',\n",
       " 'Predict',\n",
       " 'fall',\n",
       " 'rates',\n",
       " 'week',\n",
       " 'Shares',\n",
       " 'shares',\n",
       " 'Good',\n",
       " 'Should',\n",
       " 'Deal',\n",
       " 'loss',\n",
       " 'Campaign',\n",
       " 'tax',\n",
       " 'speaks',\n",
       " 'series',\n",
       " 'Word',\n",
       " 'leaks',\n",
       " 'creator',\n",
       " 'Baby',\n",
       " 'Dying',\n",
       " 'stem',\n",
       " 'Boston',\n",
       " 'Elephants',\n",
       " 'Spritz',\n",
       " 'Europe',\n",
       " 'Things',\n",
       " 'Makes',\n",
       " 'Million',\n",
       " 'Early',\n",
       " 'So',\n",
       " 'Japanese',\n",
       " 'calls',\n",
       " 'merger',\n",
       " 'plans',\n",
       " 'Releases',\n",
       " 'coming',\n",
       " 'releases',\n",
       " 'support',\n",
       " 'Reality',\n",
       " 'review',\n",
       " 'PonoMusic',\n",
       " 'Bonas',\n",
       " 'ban',\n",
       " 'Saturated',\n",
       " 'gorilla',\n",
       " 'deGrasse',\n",
       " 'Future',\n",
       " 'JetBlue',\n",
       " 'possible',\n",
       " 'cents',\n",
       " 'Shows',\n",
       " 'women',\n",
       " 'Goes',\n",
       " 'Internet',\n",
       " 'OWN',\n",
       " 'Rose',\n",
       " 'Approves',\n",
       " 'Chimerix',\n",
       " 'reading',\n",
       " 'Need',\n",
       " 'Street',\n",
       " 'Today',\n",
       " 'Year',\n",
       " 'Most',\n",
       " 'Best',\n",
       " 'People',\n",
       " 'Tech',\n",
       " 'Southwest',\n",
       " 'billion',\n",
       " 'Coming',\n",
       " 'Increase',\n",
       " 'Buy',\n",
       " 'When',\n",
       " 'tanker',\n",
       " 'Congress',\n",
       " 'OECD',\n",
       " 'Date',\n",
       " 'leaker',\n",
       " 'Mindy',\n",
       " 'Migraine',\n",
       " 'patients',\n",
       " 'Tequila',\n",
       " 'Pills',\n",
       " 'iPad',\n",
       " 'hits',\n",
       " 'February',\n",
       " 'exports',\n",
       " 'action',\n",
       " 'Just',\n",
       " 'During',\n",
       " 'second',\n",
       " 'Russian',\n",
       " 'war',\n",
       " 'Obama',\n",
       " 'Pot',\n",
       " 'If',\n",
       " 'Mae',\n",
       " 'Released',\n",
       " 'Change',\n",
       " 'She',\n",
       " 'Kaling',\n",
       " 'Marries',\n",
       " 'Linked',\n",
       " 'Obesity',\n",
       " 'Air',\n",
       " 'NHTSA',\n",
       " 'end',\n",
       " 'President',\n",
       " 'amid',\n",
       " 'posts',\n",
       " 'Last',\n",
       " 'Death',\n",
       " 'transit',\n",
       " 'wants',\n",
       " 'want',\n",
       " 'Set',\n",
       " 'UniCredit',\n",
       " 'Give',\n",
       " 'Family',\n",
       " 'Awareness',\n",
       " 'Like',\n",
       " 'Premiere',\n",
       " 'Wearable',\n",
       " 'Migraines',\n",
       " 'Medicaid',\n",
       " 'Contagious',\n",
       " 'Costco',\n",
       " 'Chromecast',\n",
       " 'app',\n",
       " 'Resolution',\n",
       " 'issues',\n",
       " 'bad',\n",
       " 'Than',\n",
       " 'Global',\n",
       " 'End',\n",
       " 'big',\n",
       " 'War',\n",
       " 'increase',\n",
       " 'There',\n",
       " 'treatment',\n",
       " 'merge',\n",
       " 'biggest',\n",
       " 'fruit',\n",
       " 'Launches',\n",
       " 'Real',\n",
       " 'gives',\n",
       " 'Rates',\n",
       " 'Virtu',\n",
       " 'star',\n",
       " 'Face',\n",
       " 'Crawley',\n",
       " 'Movie',\n",
       " 'Kickstarter',\n",
       " 'performs',\n",
       " 'Underwear',\n",
       " 'goal',\n",
       " 'fat',\n",
       " 'hospital',\n",
       " 'Diabetes',\n",
       " 'Environmental',\n",
       " 'Growth',\n",
       " 'Week',\n",
       " 'Billion',\n",
       " 'chain',\n",
       " 'Bull',\n",
       " 'Years',\n",
       " 'Record',\n",
       " 'Make',\n",
       " 'year',\n",
       " 'Worst',\n",
       " 'Into',\n",
       " 'warns',\n",
       " 'awareness',\n",
       " 'Free',\n",
       " 'fire',\n",
       " 'day',\n",
       " 'Wants',\n",
       " 'Ratings',\n",
       " 'like',\n",
       " 'Oprah',\n",
       " 'Joke',\n",
       " 'word',\n",
       " 'Headband',\n",
       " 'diabetes',\n",
       " 'Force',\n",
       " 'C',\n",
       " 'ECB',\n",
       " 'Pressure',\n",
       " 'Low',\n",
       " 'stock',\n",
       " 'Earnings',\n",
       " 'Dow',\n",
       " 'Exchange',\n",
       " 'Chapter',\n",
       " 'bitcoin',\n",
       " 'Gun',\n",
       " 'missing',\n",
       " 'Rise',\n",
       " 'Its',\n",
       " 'falls',\n",
       " 'Save',\n",
       " 'create',\n",
       " 'Fruit',\n",
       " 'Recalls',\n",
       " 'm',\n",
       " 'Inc',\n",
       " 'Announces',\n",
       " 'Q4',\n",
       " 'recreational',\n",
       " \"'re\",\n",
       " 'Chris',\n",
       " 'URBN',\n",
       " 'FX',\n",
       " 'forex',\n",
       " 'Warner',\n",
       " 'National',\n",
       " 'Secrets',\n",
       " 'Winner',\n",
       " 'Cards',\n",
       " 'Ex',\n",
       " 'Devices',\n",
       " 'prevention',\n",
       " '7-Year',\n",
       " 'Mental',\n",
       " 'San',\n",
       " 'Yawning',\n",
       " 'Mayor',\n",
       " 'section',\n",
       " 'Patch',\n",
       " 'ignition',\n",
       " 'change',\n",
       " 'EBay',\n",
       " 'Off',\n",
       " 'trading',\n",
       " 'Drop',\n",
       " 'Percent',\n",
       " 'S&P',\n",
       " 'markets',\n",
       " 'Next',\n",
       " 'MtGox',\n",
       " 'users',\n",
       " 'Mt',\n",
       " 'report',\n",
       " 'Bill',\n",
       " 'Transit',\n",
       " 'miss',\n",
       " 'Anti',\n",
       " 'Would',\n",
       " 'Ever',\n",
       " 'find',\n",
       " 'Jeep',\n",
       " 'fix',\n",
       " 'future',\n",
       " 'BOE',\n",
       " 'questions',\n",
       " 'regrets',\n",
       " 'Stop',\n",
       " 'Mac',\n",
       " 'Some',\n",
       " 'Fox',\n",
       " 'economies',\n",
       " 'Offers',\n",
       " 'Reading',\n",
       " 'WATCH',\n",
       " 'reunion',\n",
       " 'Tonight',\n",
       " 'naked',\n",
       " 'GB',\n",
       " 'baby',\n",
       " 'girlfriend',\n",
       " 'Performs',\n",
       " 'Garner',\n",
       " 'scientist',\n",
       " 'colorectal',\n",
       " 'Diego',\n",
       " 'Bad',\n",
       " 'trade',\n",
       " 'looks',\n",
       " 'watch',\n",
       " 'Decline',\n",
       " 'Weak',\n",
       " 'blog',\n",
       " 'protection',\n",
       " 'Online',\n",
       " 'Florida',\n",
       " 'launches',\n",
       " 'Prices',\n",
       " 'PM',\n",
       " 'Copper',\n",
       " 'IPO',\n",
       " 'drop',\n",
       " 'JC',\n",
       " 'online',\n",
       " 'Plant',\n",
       " 'recalled',\n",
       " 'MPs',\n",
       " 'fight',\n",
       " 'faces',\n",
       " 'Film',\n",
       " 'Service',\n",
       " 'reality',\n",
       " 'finds',\n",
       " 'Reunion',\n",
       " 'Harrison',\n",
       " 'kids',\n",
       " 'Jennifer',\n",
       " 'Science',\n",
       " 'medical',\n",
       " 'dementia',\n",
       " 'Ovarian',\n",
       " 'Hospital',\n",
       " 'Sweetener',\n",
       " 'Thomas',\n",
       " 'human',\n",
       " 'Zoo',\n",
       " 'Probe',\n",
       " 'Odyssey',\n",
       " 'iTunes',\n",
       " 'Referral',\n",
       " 'Job',\n",
       " 'EU',\n",
       " 'Know',\n",
       " 'Carl',\n",
       " 'Reports',\n",
       " 'decline',\n",
       " 'export',\n",
       " 'close',\n",
       " 'Since',\n",
       " 'Being',\n",
       " 'Herbalife',\n",
       " 'Issues',\n",
       " 'highest',\n",
       " 'discouraging',\n",
       " 'Pizza',\n",
       " 'buy',\n",
       " 'Long',\n",
       " 'See',\n",
       " 'Really',\n",
       " 'Monday',\n",
       " 'Support',\n",
       " 'offers',\n",
       " 'Biggest',\n",
       " 'surprise',\n",
       " 'plant',\n",
       " 'list',\n",
       " 'recap',\n",
       " 'start',\n",
       " 'Questions',\n",
       " 'tells',\n",
       " 'printing',\n",
       " 'Love',\n",
       " 'Sony',\n",
       " 'Two',\n",
       " 'good',\n",
       " 'Speed',\n",
       " 'Cell',\n",
       " 'fan',\n",
       " 'Pick',\n",
       " 'released',\n",
       " 'Justice',\n",
       " 'available',\n",
       " 'Possible',\n",
       " 'technology',\n",
       " 'soon',\n",
       " 'Available',\n",
       " 'streaming',\n",
       " 'Molestation',\n",
       " 'molestation',\n",
       " 'Naked',\n",
       " 'PonoPlayer',\n",
       " 'Pono',\n",
       " 'Married',\n",
       " 'Research',\n",
       " 'changes',\n",
       " 'Experimental',\n",
       " 'Cocaine',\n",
       " 'pregnancy',\n",
       " 'printed',\n",
       " 'screening',\n",
       " 'stroke',\n",
       " 'Wearables',\n",
       " 'key',\n",
       " 'Against',\n",
       " 'percent',\n",
       " 'claims',\n",
       " 'Calls',\n",
       " 'concerns',\n",
       " 'profit',\n",
       " 'anti',\n",
       " 'Markets',\n",
       " 'food',\n",
       " 'Talks',\n",
       " 'bill',\n",
       " 'firm',\n",
       " 'benefits',\n",
       " 'largest',\n",
       " 'save',\n",
       " 'millions',\n",
       " 'Emerging',\n",
       " 'copper',\n",
       " 'gains',\n",
       " 'parliament',\n",
       " 'Taxes',\n",
       " 'case',\n",
       " 'debate',\n",
       " 'steady',\n",
       " 'room',\n",
       " 'pressure',\n",
       " 'likely',\n",
       " 'Senate',\n",
       " 'man',\n",
       " 'child',\n",
       " 'young',\n",
       " 'spoilers',\n",
       " 'Tell',\n",
       " 'Wedding',\n",
       " 'song',\n",
       " \"'m\",\n",
       " 'Their',\n",
       " 'Care',\n",
       " 'Makers',\n",
       " 'underwear',\n",
       " 'devices',\n",
       " 'headaches',\n",
       " 'Patients',\n",
       " 'Smoking',\n",
       " 'Concept',\n",
       " 'Spacetime',\n",
       " 'Read',\n",
       " 'Chairman',\n",
       " 'low',\n",
       " 'Forex',\n",
       " 'talk',\n",
       " 'investors',\n",
       " 'Trading',\n",
       " 'urges',\n",
       " 'boost',\n",
       " 'dip',\n",
       " 'Five',\n",
       " 'An',\n",
       " 'Look',\n",
       " 'Major',\n",
       " 'slip',\n",
       " 'Fall',\n",
       " 'halt',\n",
       " 'seeks',\n",
       " 'reveals',\n",
       " 'spread',\n",
       " 'crash',\n",
       " 'government',\n",
       " 'Say',\n",
       " 'service',\n",
       " 'Highest',\n",
       " 'Increased',\n",
       " 'Between',\n",
       " 'stop',\n",
       " 'need',\n",
       " 'raise',\n",
       " 'set',\n",
       " 'based',\n",
       " 'Rate',\n",
       " 'Missing',\n",
       " 'crashes',\n",
       " 'collects',\n",
       " 'Want',\n",
       " 'Profit',\n",
       " 'Gives',\n",
       " 'Scotland',\n",
       " 'scandal',\n",
       " 'promises',\n",
       " 'Room',\n",
       " 'return',\n",
       " 'Case',\n",
       " 'Sign',\n",
       " 'look',\n",
       " 'dressing',\n",
       " 'Surprise',\n",
       " 'Player',\n",
       " 'film',\n",
       " 'Verizon',\n",
       " 'electric',\n",
       " 'Medication',\n",
       " 'medicine',\n",
       " 'predicts',\n",
       " 'breakthrough',\n",
       " 'voices',\n",
       " 'ovarian',\n",
       " 'Gestational',\n",
       " 'Stroke',\n",
       " 'yawning',\n",
       " 'Siri',\n",
       " 'euro',\n",
       " 'banks',\n",
       " 'eBay',\n",
       " 'Picks',\n",
       " 'Investors',\n",
       " 'Food',\n",
       " 'Results',\n",
       " 'Lower',\n",
       " 'Lead',\n",
       " 'Despite',\n",
       " 'turns',\n",
       " 'Bitcoins',\n",
       " 'Second',\n",
       " 'Legal',\n",
       " 'killed',\n",
       " 'Four',\n",
       " 'adds',\n",
       " 'numbers',\n",
       " 'America',\n",
       " 'Exports',\n",
       " 'Protect',\n",
       " 'Crisis',\n",
       " 'Plans',\n",
       " 'Take',\n",
       " 'Brands',\n",
       " 'Hong',\n",
       " 'Or',\n",
       " 'recalls',\n",
       " 'Ohio',\n",
       " 'port',\n",
       " 'Tanker',\n",
       " 'lead',\n",
       " 'They',\n",
       " 'Let',\n",
       " 'RBS',\n",
       " 'Scandal',\n",
       " 'Details',\n",
       " 'Likely',\n",
       " 'Program',\n",
       " 'arrested',\n",
       " 'Predicts',\n",
       " 'Streaming',\n",
       " 'joke',\n",
       " 'parents',\n",
       " 'Festival',\n",
       " 'Seth',\n",
       " 'Rogen',\n",
       " 'Technology',\n",
       " 'Users',\n",
       " 'debut',\n",
       " 'Videos',\n",
       " 'Fat',\n",
       " 'hopes',\n",
       " 'research',\n",
       " 'Approved',\n",
       " 'Thousands',\n",
       " 'Fats',\n",
       " 'researchers',\n",
       " 'Stem',\n",
       " 'weight',\n",
       " 'cigarettes',\n",
       " 'e',\n",
       " 'age',\n",
       " 'gel',\n",
       " 'Link',\n",
       " 'Kirkland',\n",
       " 'fats',\n",
       " 'saturated',\n",
       " 'install',\n",
       " 'Nexus',\n",
       " 'Ignition',\n",
       " 'AT&T',\n",
       " 'fixes',\n",
       " 'gallon',\n",
       " 'sees',\n",
       " 'Ahead',\n",
       " 'worries',\n",
       " 'board',\n",
       " 'Drops',\n",
       " 'Futures',\n",
       " 'Amid',\n",
       " 'Little',\n",
       " 'Where',\n",
       " 'class',\n",
       " 'Daily',\n",
       " 'Post',\n",
       " 'Lehman',\n",
       " 'money',\n",
       " 'Digital',\n",
       " 'train',\n",
       " 'York',\n",
       " 'Ackman',\n",
       " 'Due',\n",
       " 'Using',\n",
       " 'Sees',\n",
       " 'Recovery',\n",
       " 'economy',\n",
       " 'crisis',\n",
       " 'White',\n",
       " 'response',\n",
       " 'Chrysler',\n",
       " 'Kong',\n",
       " 'hold',\n",
       " 'Financial',\n",
       " 'Bon',\n",
       " 'Ton',\n",
       " 'J.C.',\n",
       " 'Great',\n",
       " 'Department',\n",
       " 'escapes',\n",
       " 'bring',\n",
       " 'Posts',\n",
       " 'Loss',\n",
       " 'committee',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema!\n",
    "\n",
    "Tengo los words embeddings y puedo agruparlos por cluster, pero lo que en verdad quiero categorizar (clusterizar) son los titulos (sentencia completa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2, 8000), indices imply (2, 10006)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4872\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4873\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4874\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4842\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4843\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (2, 8000), indices imply (2, 10006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-375b2e24c96b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m news = { 'title': new_data['TITLE'].tolist(), 'category': new_data['CATEGORY'].tolist(),\n\u001b[1;32m      2\u001b[0m         'cluster': assigned_clusters}\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0massigned_clusters\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7321\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7323\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4875\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4877\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4841\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4842\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4843\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (2, 8000), indices imply (2, 10006)"
     ]
    }
   ],
   "source": [
    "news = { 'title': new_data['TITLE'].tolist(), 'category': new_data['CATEGORY'].tolist(),\n",
    "        'cluster': assigned_clusters}\n",
    "news = pd.DataFrame(news, index = [assigned_clusters] , columns = ['title', 'category'])\n",
    "print(news.loc[0].groupby('category').count())\n",
    "print(news.loc[1].groupby('category').count())\n",
    "print(news.loc[2].groupby('category').count())\n",
    "print(news.loc[3].groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
