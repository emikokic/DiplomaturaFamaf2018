{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo Práctico 1 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiploDatos 2018 - Aprendizaje No Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mario Ferreyra - Emiliano Kokic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos: Dataset of references (urls) to news web pages\n",
    "https://archive.ics.uci.edu/ml/datasets/News+Aggregator\n",
    "\n",
    "Descripción del problema: el dataset contiene dentro de sus atributos título y categoria de noticia:\n",
    "- Entretenimiento\n",
    "- Ciencia y Tecnología\n",
    "- Negocios\n",
    "- Salud\n",
    "\n",
    "La idea del problema sería intentar aplicar técnicas de clustering sobre los títulos de cada noticia y ver si de acuerdo a su semántica se agrupan según las distintas categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from gensim.models import Word2Vec, Doc2Vec, KeyedVectors\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nID          Numeric ID\\nTITLE       News title \\nURL         Url\\nPUBLISHER   Publisher name\\nCATEGORY    News category (b = business, t = science and technology, e = entertainment, m = health)\\nSTORY       Alphanumeric ID of the cluster that includes news about the same story\\nHOSTNAME    Url hostname\\nTIMESTAMP   Approximate time the news was published, as the number of milliseconds since the epoch\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./NewsAggregatorDataset/newsCorpora.csv', header=None, delimiter='\\t')\n",
    "data.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "'''\n",
    "ID          Numeric ID\n",
    "TITLE       News title \n",
    "URL         Url\n",
    "PUBLISHER   Publisher name\n",
    "CATEGORY    News category (b = business, t = science and technology, e = entertainment, m = health)\n",
    "STORY       Alphanumeric ID of the cluster that includes news about the same story\n",
    "HOSTNAME    Url hostname\n",
    "TIMESTAMP   Approximate time the news was published, as the number of milliseconds since the epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  Fed official says weak data caused by weather,...        b\n",
       "1  Fed's Charles Plosser sees high bar for change...        b\n",
       "2  US open: Stocks fall after Fed official hints ...        b\n",
       "3  Fed risks falling 'behind the curve', Charles ...        b\n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos quedamos solo con las dos columnas de interés\n",
    "data = data[['TITLE', 'CATEGORY']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITLE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The article requested cannot be found! Please refresh your browser or go back  ...</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Highlights</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Posted by Parvez Jabri</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Posted by Imaduddin</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Posted by Shoaib-ur-Rehman Siddiqui</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    CATEGORY\n",
       "TITLE                                                       \n",
       "The article requested cannot be found! Please r...       145\n",
       "Business Highlights                                       59\n",
       "Posted by Parvez Jabri                                    59\n",
       "Posted by Imaduddin                                       53\n",
       "Posted by Shoaib-ur-Rehman Siddiqui                       52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = data.groupby('TITLE').count().sort_values('CATEGORY',ascending=False)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos solo con los títulos que ocurren una sola vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oracle shares slide on weaker-than-expected Q3 results',\n",
       " 'Only one kind of inflation matters',\n",
       " \"Only one large bank fails Fed's stress test\",\n",
       " \"Original 'Star Wars' stars Harrison Ford, Mark Hamill and Carrie Fisher officially  ...\",\n",
       " \"One of America's First 'Bionic Eyes' Goes to a Former Weightlifter in Michigan\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_titles = grouped[grouped.CATEGORY == 1].index.values.tolist()\n",
    "unique_titles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.TITLE.isin(unique_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>108143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>142220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>41908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>101128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TITLE\n",
       "CATEGORY        \n",
       "b         108143\n",
       "e         142220\n",
       "m          41908\n",
       "t         101128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('CATEGORY').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con 10000 títulos de cada categoría (ya que sino se necesita demasiado cómputo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TITLE\n",
       "CATEGORY       \n",
       "b         10000\n",
       "e         10000\n",
       "m         10000\n",
       "t         10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data[data.CATEGORY == 'b'][0:10000]\n",
    "new_data = new_data.append(data[data.CATEGORY == 'e'][0:10000], ignore_index=True)\n",
    "new_data = new_data.append(data[data.CATEGORY == 'm'][0:10000], ignore_index=True)\n",
    "new_data = new_data.append(data[data.CATEGORY == 't'][0:10000], ignore_index=True)\n",
    "new_data.groupby('CATEGORY').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es entonces aplicar clustering sobre los títulos de las noticias y ver si se agrupan por categoría de acuerdo a su semantica. Es decir, vamos a considerar 4 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del texto | tokenizing\n",
    "\n",
    "Para analizar el texto debemos estudiar la frecuencia de las palabras, es decir, separar el texto en unidades sintácticas o *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_tokenized = []\n",
    "\n",
    "for index, row in new_data.iterrows():\n",
    "    allwords_tokenized = tokenize_only(row['TITLE'])\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay en total 361184 tokens \n",
      "\n",
      "['fed', 'official', 'says', 'weak', 'data', 'caused', 'by', 'weather', 'should', 'not', 'slow', 'taper', 'fed', \"'s\", 'charles', 'plosser', 'sees', 'high', 'bar', 'for', 'change', 'in', 'pace', 'of', 'tapering', 'us', 'open', 'stocks', 'fall', 'after', 'fed', 'official', 'hints', 'at', 'accelerated', 'tapering', 'fed', 'risks', 'falling', \"'behind\", 'the', 'curve', 'charles', 'plosser', 'says', 'fed', \"'s\", 'plosser', 'nasty', 'weather']\n"
     ]
    }
   ],
   "source": [
    "print('Hay en total ' + str(len(totalvocab_tokenized)) + ' tokens \\n')\n",
    "len(totalvocab_tokenized)\n",
    "print (totalvocab_tokenized[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 277162)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    tokenizer=tokenize_only,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(new_data['TITLE'])\n",
    "print(tfidf_matrix.shape)\n",
    "# terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar clusters | Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 1832 elementos\n",
      "El cluster 1 tiene 5572 elementos\n",
      "El cluster 2 tiene 32067 elementos\n",
      "El cluster 3 tiene 529 elementos\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "# print (clusters)\n",
    "\n",
    "# Recuento del número de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          title\n",
      "category       \n",
      "e           855\n",
      "m           772\n",
      "t           205\n",
      "          title\n",
      "category       \n",
      "b          1396\n",
      "e          1989\n",
      "m           986\n",
      "t          1201\n",
      "          title\n",
      "category       \n",
      "b          8604\n",
      "e          6627\n",
      "m          8242\n",
      "t          8594\n",
      "          title\n",
      "category       \n",
      "e           529\n"
     ]
    }
   ],
   "source": [
    "news = {\n",
    "    'title': new_data['TITLE'].tolist(),\n",
    "    'category': new_data['CATEGORY'].tolist(),\n",
    "    'cluster': clusters\n",
    "}\n",
    "news = pd.DataFrame(news, index = [clusters] , columns = ['title', 'category'])\n",
    "\n",
    "print(news.loc[0].groupby('category').count())\n",
    "print(news.loc[1].groupby('category').count())\n",
    "print(news.loc[2].groupby('category').count())\n",
    "print(news.loc[3].groupby('category').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que uno de los clusters concentra casi la totalidad de los datos, sumado al hecho de que las categorias no parecen agruparse en distintos clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probemos utilizando el tokenizer de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTA: token.is_stop nos devuelve un booleano que indica si el token es o no una stopword\n",
    "# NOTA: en consola ejecutar la primera vez: python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)\n",
    "tokens = []\n",
    "for doc in tokenizer.pipe(new_data['TITLE']):\n",
    "    tokens.append([token.text for token in doc if re.search('[a-zA-Z]', token.text)\n",
    "                   and not token.is_stop])    \n",
    "# tokens es una lista de listas donde cada lista contiene los tokens de cada titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fed',\n",
       " 'official',\n",
       " 'says',\n",
       " 'weak',\n",
       " 'data',\n",
       " 'caused',\n",
       " 'weather',\n",
       " 'slow',\n",
       " 'taper']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probemos utilizando Gensim para generar word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: la dimensionalidad de los vectores generados por Gensim es por defecto igual a 100.\n",
    "# Esto se puede cambiar utilizando el parámetro 'size' del método Word2Vec\n",
    "model = Word2Vec(tokens, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25262"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para generar el vector asociado a una oración probemos sumando los vectores de las palabras que la componen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vector = []\n",
    "for i in range(len(tokens)):\n",
    "    sen_vector.append(sum([word_vectors.get_vector(token) for token in tokens[i]]))\n",
    "\n",
    "new_data['sen_vector'] = sen_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.9365424e-01,  1.7465535e+00, -5.6100558e-03, -7.4673064e-02,\n",
       "       -1.0276629e+00, -1.5303760e+00, -4.2149308e-01, -1.1297854e+00,\n",
       "        2.4284315e+00,  2.0443189e+00,  6.7305312e-02, -2.3876677e+00,\n",
       "       -2.1749310e+00, -1.4219019e+00,  3.2185075e+00, -2.7297885e+00,\n",
       "        3.5554972e+00,  1.3318231e+00, -5.6807715e-01, -7.2824962e-02,\n",
       "        4.6024850e-01,  4.2863836e+00,  3.2643213e+00, -2.4070446e+00,\n",
       "        2.4743659e+00, -1.5242141e+00,  3.9516438e-02, -2.9862692e+00,\n",
       "        1.1444232e+00, -1.9588170e+00,  2.5828264e+00, -9.8179710e-01,\n",
       "        5.8011436e+00, -2.1580663e+00, -2.2712402e+00,  2.5019841e+00,\n",
       "       -4.1087584e+00, -3.4930363e+00,  2.0053573e+00, -5.8574090e+00,\n",
       "        1.6151816e-01,  6.4584394e+00, -2.7966452e-01,  2.1548975e+00,\n",
       "        8.5283279e-01, -2.3360028e+00, -3.7928710e+00, -4.5333138e+00,\n",
       "        1.9424528e+00,  9.6115440e-01,  1.3376893e+00,  3.3069875e+00,\n",
       "        2.4346318e+00, -1.8079658e+00, -8.3988035e-01,  4.5729880e+00,\n",
       "        1.8991697e+00, -3.0933827e-01,  3.2115815e+00,  2.2495861e+00,\n",
       "       -3.0854464e-01,  1.2338892e+00,  3.4383955e+00, -1.4320192e+00,\n",
       "        1.2291019e+00,  1.7289286e+00, -6.8438441e-01, -1.6093272e+00,\n",
       "        6.8356633e-01,  1.8969835e+00,  7.5800581e+00, -1.3089885e+00,\n",
       "        1.6931386e+00, -8.0023503e-01, -1.9968020e+00,  3.8478515e+00,\n",
       "        1.5992852e+00, -3.6217928e+00, -2.2387738e+00, -1.3234560e+00,\n",
       "        9.3290854e-01,  3.3703001e+00, -1.6880825e+00, -4.1487603e+00,\n",
       "       -2.1325995e-01, -4.6496935e+00,  4.0203199e+00, -8.1517100e-02,\n",
       "        2.5688205e+00,  3.2343271e+00, -3.6344662e+00,  1.9683741e+00,\n",
       "       -7.3813045e-01,  2.7604055e+00,  2.8492787e+00, -2.4047501e+00,\n",
       "        1.9848064e+00,  6.1579651e-01,  5.6187406e+00, -3.1339124e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['sen_vector'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilicemos KMeans de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance)\n",
    "assigned_clusters = kclusterer.cluster(new_data['sen_vector'].values, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 15649 elementos\n",
      "El cluster 1 tiene 1482 elementos\n",
      "El cluster 2 tiene 14900 elementos\n",
      "El cluster 3 tiene 7969 elementos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(NUM_CLUSTERS):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, assigned_clusters.count(i)))\n",
    "\n",
    "len(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          title\n",
      "category       \n",
      "b          3058\n",
      "e          4454\n",
      "m          3099\n",
      "t          5038\n",
      "          title\n",
      "category       \n",
      "b          1239\n",
      "e           133\n",
      "m            75\n",
      "t            35\n",
      "          title\n",
      "category       \n",
      "b          4392\n",
      "e          2102\n",
      "m          5377\n",
      "t          3029\n",
      "          title\n",
      "category       \n",
      "b          1311\n",
      "e          3311\n",
      "m          1449\n",
      "t          1898\n"
     ]
    }
   ],
   "source": [
    "news = {\n",
    "    'title': new_data['TITLE'].tolist(),\n",
    "    'category': new_data['CATEGORY'].tolist(),\n",
    "    'cluster': assigned_clusters\n",
    "}\n",
    "news = pd.DataFrame(\n",
    "    news,\n",
    "    index=[assigned_clusters],\n",
    "    columns = ['title', 'category']\n",
    ")\n",
    "\n",
    "print(news.loc[0].groupby('category').count())\n",
    "print(news.loc[1].groupby('category').count())\n",
    "print(news.loc[2].groupby('category').count())\n",
    "print(news.loc[3].groupby('category').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se obtienen buenos resultados ya sea usando distancia Euclídea o distancia Coseno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomamos el promedio de los vectores de palabras que ocurren en una oración y eso lo utilizamos como el vector que representa esa oración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vector = []\n",
    "for i in range(len(tokens)):\n",
    "    sen_vector.append(np.mean([word_vectors.get_vector(token) for token in tokens[i]], axis=0))\n",
    "\n",
    "new_data['sen_vector'] = sen_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 1559 elementos\n",
      "El cluster 1 tiene 18205 elementos\n",
      "El cluster 2 tiene 6518 elementos\n",
      "El cluster 3 tiene 13718 elementos\n",
      "          title\n",
      "category       \n",
      "b          1270\n",
      "e           134\n",
      "m           108\n",
      "t            47\n",
      "          title\n",
      "category       \n",
      "b          5101\n",
      "e          2880\n",
      "m          5871\n",
      "t          4353\n",
      "          title\n",
      "category       \n",
      "b           661\n",
      "e          3177\n",
      "m          1022\n",
      "t          1658\n",
      "          title\n",
      "category       \n",
      "b          2968\n",
      "e          3809\n",
      "m          2999\n",
      "t          3942\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance)\n",
    "assigned_clusters = kclusterer.cluster(new_data['sen_vector'].values, assign_clusters=True)\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, assigned_clusters.count(i)))\n",
    "\n",
    "len(assigned_clusters)\n",
    "\n",
    "news = {\n",
    "    'title': new_data['TITLE'].tolist(),\n",
    "    'category': new_data['CATEGORY'].tolist(),\n",
    "    'cluster': assigned_clusters\n",
    "}\n",
    "news = pd.DataFrame(\n",
    "    news,\n",
    "    index=[assigned_clusters],\n",
    "    columns = ['title', 'category']\n",
    ")\n",
    "\n",
    "print(news.loc[0].groupby('category').count())\n",
    "print(news.loc[1].groupby('category').count())\n",
    "print(news.loc[2].groupby('category').count())\n",
    "print(news.loc[3].groupby('category').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente no obtenemos buenos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos cercanos al centroide de cada Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 277162)\n",
      "El cluster 0 tiene 33322 elementos\n",
      "El cluster 1 tiene 529 elementos\n",
      "El cluster 2 tiene 5761 elementos\n",
      "El cluster 3 tiene 388 elementos\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    tokenizer=tokenize_only,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(new_data['TITLE'])\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# print (clusters)\n",
    "\n",
    "# Recuento del número de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_transformed = (40000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99944658, 1.04142837, 1.00314848, 1.04209393],\n",
       "       [0.99990217, 1.04116972, 0.99875999, 1.04234858],\n",
       "       [0.99966024, 1.04142866, 1.00331806, 1.04192938],\n",
       "       ...,\n",
       "       [1.00008886, 1.04152   , 1.00340228, 1.04158144],\n",
       "       [0.99988267, 1.04171564, 1.00365004, 1.04113871],\n",
       "       [0.99889601, 1.03892275, 1.00276408, 1.04104361]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform X to a cluster-distance space.\n",
    "# In the new space, each dimension is the distance to the cluster centers.\n",
    "# Note that even if X is sparse, the array returned by transform will typically be dense.\n",
    "\n",
    "X_transformed = km.transform(tfidf_matrix)\n",
    "print(\"Shape X_transformed = {}\".format(X_transformed.shape))\n",
    "display(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get closest titles to centroid in cluster\n",
    "def get_closest_titles(X_trans, clust, max_t=10):\n",
    "    idx = np.argsort(X_trans[:, clust])[::-1][:50]\n",
    "    #titles = data['TITLE'].iloc[idx].drop_duplicates()\n",
    "    titles = data.iloc[idx].drop_duplicates()\n",
    "\n",
    "    return titles[:max_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCluster 0\n",
      "\tTitles and Categories:\n",
      "                                                   TITLE CATEGORY\n",
      "16120  Two Weeks Remain for Healthcare Enrollment in ...        b\n",
      "13121  [Weekend Poll] Are You Keeping Your Amazon Pri...        b\n",
      "10920  American Idol: Jena Irene, C.J. Harris and Ale...        e\n",
      "22791  Stones cancel Aussie tour following Scott's death        e\n",
      "23785  Study Doubts Saturated Fat's Link to Heart Dis...        m\n",
      "\n",
      "\n",
      "\tCluster 1\n",
      "\tTitles and Categories:\n",
      "                                                   TITLE CATEGORY\n",
      "23251  Danica McKellar Dances Foxtrot For Week 1 Of DWTS        e\n",
      "10369  Happy birthday World Wide Web! Why the D.C. re...        t\n",
      "27938              Oprah creates tea drink for Starbucks        e\n",
      "34023  Half of breast cancer surgeries in UK not need...        m\n",
      "30174  Mt. Gox suddenly finds 200000 missing bitcoins...        b\n",
      "\n",
      "\n",
      "\tCluster 2\n",
      "\tTitles and Categories:\n",
      "                                                   TITLE CATEGORY\n",
      "21971  Dealing with compact car recall, GM CEO apolog...        t\n",
      "34831  Missing plane timeline: The search for Malaysi...        b\n",
      "30764         Google Goes Mythbusting About Google Glass        t\n",
      "4490   Italy's UniCredit posts record loss on bad loa...        b\n",
      "655                                Window on Westminster        b\n",
      "\n",
      "\n",
      "\tCluster 3\n",
      "\tTitles and Categories:\n",
      "                                                   TITLE CATEGORY\n",
      "32191                       When Ms Fey meets Miss Piggy        e\n",
      "32574  L'Wren Scott Death: Late Fashion Designer's Fu...        e\n",
      "7565   The Originals (Video) – Season 1 Episode 16 “F...        e\n",
      "13949  Microsoft exec: Despite rumors, Xbox One contr...        t\n",
      "30659  Toyota to pay $1.2 billion fine for misleading...        t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_clust in range(num_clusters):\n",
    "    closest_titles_to_centroid = get_closest_titles(X_transformed, n_clust, max_t=5)\n",
    "    print('\\tCluster {}'.format(n_clust))\n",
    "    print('\\tTitles and Categories:')\n",
    "    print(closest_titles_to_centroid)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probemos utilizando el modelo word2vec pre-entrenado de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando KeyedVectors de Gensim para cargar el modelo tiene la desventaja de que no se puede seguir entrenando. Pero es más eficiente que utilizar gensim.models.Word2Vec\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demora: 10.572213649749756\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# model = KeyedVectors.load_word2vec_format('/users/ekokic/thesis/models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.save('/users/ekokic/thesis/models/word2vecGoogle.model')\n",
    "model = KeyedVectors.load('/users/ekokic/thesis/models/word2vecGoogle.model')\n",
    "end = time.time()\n",
    "print('demora: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de word embeddings: 3000000\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de word embeddings: {}'.format(len(model.vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de los vectores: 300\n"
     ]
    }
   ],
   "source": [
    "print('Dimensionalidad de los vectores: {}'.format(model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras que no están en el vocabulario del modelo: 9920\n"
     ]
    }
   ],
   "source": [
    "sen_vector = []\n",
    "count = 0\n",
    "for i in range(len(tokens)):\n",
    "    try:\n",
    "        sen_vector.append(sum([model.get_vector(token) for token in tokens[i]]))\n",
    "    except Exception:\n",
    "        sen_vector.append(np.nan)\n",
    "        count += 1\n",
    "print('Cantidad de palabras que no están en el vocabulario del modelo: {}'.format(count))        \n",
    "\n",
    "new_data['sen_vector'] = sen_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>sen_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>b</td>\n",
       "      <td>[0.29174805, 0.81469727, -0.42895508, 0.797119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>b</td>\n",
       "      <td>[-0.42236328, 0.5252075, -0.58447266, 1.198486...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>b</td>\n",
       "      <td>[0.033691406, 0.782959, -0.8574219, 1.7483215,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY  \\\n",
       "0  Fed official says weak data caused by weather,...        b   \n",
       "1  Fed's Charles Plosser sees high bar for change...        b   \n",
       "2  US open: Stocks fall after Fed official hints ...        b   \n",
       "3  Fed risks falling 'behind the curve', Charles ...        b   \n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b   \n",
       "\n",
       "                                          sen_vector  \n",
       "0  [0.29174805, 0.81469727, -0.42895508, 0.797119...  \n",
       "1                                                NaN  \n",
       "2  [-0.42236328, 0.5252075, -0.58447266, 1.198486...  \n",
       "3  [0.033691406, 0.782959, -0.8574219, 1.7483215,...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar el problema vamos a dropear aquellas oraciones que contienen palabras que no están en el vocabulario. \n",
    "\n",
    "Lo ideal sería entrenar el modelo pre-entrenado con este corpus así se actualiza el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>sen_vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>7435</td>\n",
       "      <td>7435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>7007</td>\n",
       "      <td>7007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>8394</td>\n",
       "      <td>8394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TITLE  sen_vector\n",
       "CATEGORY                   \n",
       "b          7435        7435\n",
       "e          7007        7007\n",
       "m          8394        8394\n",
       "t          7244        7244"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.groupby('CATEGORY').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 4211 elementos\n",
      "El cluster 1 tiene 6438 elementos\n",
      "El cluster 2 tiene 9512 elementos\n",
      "El cluster 3 tiene 9919 elementos\n",
      "          title\n",
      "category       \n",
      "b           724\n",
      "e            80\n",
      "m            84\n",
      "t          3323\n",
      "          title\n",
      "category       \n",
      "b           205\n",
      "e          5006\n",
      "m           570\n",
      "t           657\n",
      "          title\n",
      "category       \n",
      "b          2619\n",
      "e          1731\n",
      "m          3114\n",
      "t          2048\n",
      "          title\n",
      "category       \n",
      "b          3887\n",
      "e           190\n",
      "m          4626\n",
      "t          1216\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance)\n",
    "assigned_clusters = kclusterer.cluster(new_data['sen_vector'].values, assign_clusters=True)\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, assigned_clusters.count(i)))\n",
    "\n",
    "len(assigned_clusters)\n",
    "\n",
    "news = {\n",
    "    'title': new_data['TITLE'].tolist(),\n",
    "    'category': new_data['CATEGORY'].tolist(),\n",
    "    'cluster': assigned_clusters\n",
    "}\n",
    "news = pd.DataFrame(\n",
    "    news,\n",
    "    index=[assigned_clusters],\n",
    "    columns = ['title', 'category']\n",
    ")\n",
    "\n",
    "print(news.loc[0].groupby('category').count())\n",
    "print(news.loc[1].groupby('category').count())\n",
    "print(news.loc[2].groupby('category').count())\n",
    "print(news.loc[3].groupby('category').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "En nuestro caso particular aplicar clustering no obtuvo buenos resultados. Probablemente al utilizar solamente los títulos no se aporta suficiente información. Además, dichos títulos pueden contener palabras similares pero que correspondan a categorias de noticias distintas (ambigüedad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
