{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 2\n",
    "\"\"\"\n",
    "import pandas\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import keras.backend as K\n",
    "# K is just another name for the keras backend: tensorflow (or theaso,\n",
    "# if you are using a different backend).\n",
    "from keras.layers import Embedding, Average, Lambda, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import utils, optimizers, regularizers\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from utils import FilteredFastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = load_files('./dataset/review_polarity/txt_sentoken',\n",
    "                         shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.data,\n",
    "                                                        dataset.target,\n",
    "                                                        test_size=0.1, # antes estaba en 0.25\n",
    "                                                        random_state=42)\n",
    "    print('Training samples {}, test_samples {}'\n",
    "          .format(len(X_train), len(X_test)))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"\n",
    "    Replaces the words in instances with their index in mapping.\n",
    "\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, max_text_length).\n",
    "    \"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        word_indices.append([mapping[word.decode('utf-8')]\n",
    "                             for word in instance.split()])\n",
    "\n",
    "    # Check consistency\n",
    "    assert len(instances[0].split()) == len(word_indices[0])\n",
    "\n",
    "    # Pad the sequences to obtain a matrix instead of a list of lists.\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    return pad_sequences(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples 1800, test_samples 200\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# args = read_args()\n",
    "args = {'num_units': 100, 'dropout': 0.5, 'batch_size': 32, 'epochs': 10,\n",
    "        'experiment_name': 'mlp_test', 'embeddings_filename': 'filteredFastText'}\n",
    "X_train, X_test, y_train, y_test_original = load_dataset()\n",
    "# TODO 1: Convert the labels to categorical -- DONE\n",
    "num_classes = 2 # POS and NEG\n",
    "print(y_train[0])\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test_original = utils.to_categorical(y_test_original, num_classes)\n",
    "# Load the filtered FastText word vectors, using only the vocabulary in\n",
    "# the movie reviews dataset\n",
    "with open(args['embeddings_filename'], 'rb') as model_file:\n",
    "    filtered_fasttext = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "len(filtered_fasttext.get_vector('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'robert', b'redford', b'is', b'very', b'good']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].split()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next thing to do is to choose how we are going to represent our\n",
    "# training matrix. Each review must be translated into a single vector.\n",
    "# This means we have to combine, somehow, the word vectors of each\n",
    "# word in the review. Some options are:\n",
    "#  - Take the average of all vectors.\n",
    "#  - Take the minimum and maximum value of each feature.\n",
    "# All these operations are vectorial and easier to compute using a GPU.\n",
    "# Then, it is better to put them inside the Keras model.\n",
    "\n",
    "# The Embedding layer will be quite handy in solving this problem for us.\n",
    "# To use this layer, the input to the network has to be the indices of the\n",
    "# words on the embedding matrix.\n",
    "X_train = transform_input(X_train, filtered_fasttext.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input is ready, start the model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        filtered_fasttext.wv.shape[0],  # Vocabulary size\n",
    "        filtered_fasttext.wv.shape[1],  # Embedding size\n",
    "        weights=[filtered_fasttext.wv],  # Word vectors\n",
    "        trainable=False  # This indicates the word vectors must not be\n",
    "    )                    # changed during training.\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "The output here has shape\n",
    "    (batch_size (?), words_in_reviews (?), embedding_size)\n",
    "To use a Dense layer, the input must have only 2 dimensions. We need to\n",
    "create a single representation for each document, combining the word\n",
    "embeddings of the words in the instance.\n",
    "For this, we have to use a Tensorflow (K) operation directly.\n",
    "The operation we need to do is to take the average of the embeddings\n",
    "on the second dimension. We wrap this operation on a Lambda\n",
    "layer to include it into the model.\n",
    "\"\"\"\n",
    "model.add(\n",
    "    Lambda(lambda xin: K.mean(xin, axis=1), name='embedding_average')\n",
    "#     Lambda(lambda xin: K.concatenate([K.min(xin, axis=1), K.max(xin, axis=1)]),\n",
    "#            name='embedding_min_max')\n",
    ")\n",
    "# Now the output shape is (batch_size (?), embedding_size)\n",
    "\n",
    "# TODO 2: Finish the Keras model\n",
    "# Add all the layers\n",
    "\n",
    "model.add(\n",
    "    Dense(10, activation='relu')\n",
    ")\n",
    "# model.add(\n",
    "#     Dropout(0.5)\n",
    "# )\n",
    "model.add(\n",
    "    Dense(10, activation='relu')\n",
    ")\n",
    "model.add(\n",
    "    Dense(1, activation='sigmoid')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, None, 300)         15276000  \n",
      "_________________________________________________________________\n",
      "embedding_average (Lambda)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 15,279,131\n",
      "Trainable params: 3,131\n",
      "Non-trainable params: 15,276,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', # probar categorical sino\n",
    "              optimizer=optimizers.Adagrad(lr=0.001, decay=0.0001), \n",
    "              # También podría ser el string \"Adagrad\" con los parámetros por defecto\n",
    "              metrics=['accuracy'])  # La métrica sirve para llevar algún registro además del costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1620 samples, validate on 180 samples\n",
      "Epoch 1/10\n",
      "1620/1620 [==============================] - 2s 1ms/step - loss: 0.6915 - acc: 0.5222 - val_loss: 0.6897 - val_acc: 0.5389\n",
      "Epoch 2/10\n",
      "1620/1620 [==============================] - 1s 724us/step - loss: 0.6905 - acc: 0.5531 - val_loss: 0.6894 - val_acc: 0.5500\n",
      "Epoch 3/10\n",
      "1620/1620 [==============================] - 1s 743us/step - loss: 0.6904 - acc: 0.5512 - val_loss: 0.6893 - val_acc: 0.5444\n",
      "Epoch 4/10\n",
      "1620/1620 [==============================] - 1s 733us/step - loss: 0.6902 - acc: 0.5481 - val_loss: 0.6893 - val_acc: 0.5389\n",
      "Epoch 5/10\n",
      "1620/1620 [==============================] - 1s 724us/step - loss: 0.6902 - acc: 0.5481 - val_loss: 0.6892 - val_acc: 0.5444\n",
      "Epoch 6/10\n",
      "1620/1620 [==============================] - 1s 716us/step - loss: 0.6901 - acc: 0.5537 - val_loss: 0.6891 - val_acc: 0.5444\n",
      "Epoch 7/10\n",
      "1620/1620 [==============================] - 1s 720us/step - loss: 0.6901 - acc: 0.5525 - val_loss: 0.6891 - val_acc: 0.5333\n",
      "Epoch 8/10\n",
      "1620/1620 [==============================] - 1s 732us/step - loss: 0.6900 - acc: 0.5568 - val_loss: 0.6890 - val_acc: 0.5333\n",
      "Epoch 9/10\n",
      "1620/1620 [==============================] - 1s 722us/step - loss: 0.6900 - acc: 0.5556 - val_loss: 0.6890 - val_acc: 0.5333\n",
      "Epoch 10/10\n",
      "1620/1620 [==============================] - 1s 735us/step - loss: 0.6899 - acc: 0.5611 - val_loss: 0.6890 - val_acc: 0.5333\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Fit the model -- DONE\n",
    "history = model.fit(X_train, y_train, batch_size=args['batch_size'], epochs=10,\n",
    "                    validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.7438    , -0.1831    ,  0.2255    , ..., -0.0414    ,\n",
       "          0.2202    ,  0.0802    ],\n",
       "        [-0.0995    , -0.034     , -0.2414    , ...,  0.0221    ,\n",
       "         -0.3485    , -0.0184    ],\n",
       "        [ 0.02206548, -0.3473758 , -1.1992983 , ..., -0.0711825 ,\n",
       "          0.36806315, -2.5473688 ],\n",
       "        ...,\n",
       "        [-0.1464    ,  0.0687    , -0.1861    , ..., -0.1806    ,\n",
       "          0.1267    ,  0.1475    ],\n",
       "        [ 0.2922    , -0.2796    , -0.6978    , ..., -0.0651    ,\n",
       "          0.3773    ,  0.2411    ],\n",
       "        [-0.3755    ,  0.189     ,  0.1702    , ...,  0.1788    ,\n",
       "         -0.1416    , -0.1588    ]], dtype=float32),\n",
       " array([[ 0.02506906,  0.10300997,  0.09203143, ...,  0.03028385,\n",
       "          0.00413218, -0.01831461],\n",
       "        [ 0.04128695, -0.0692794 ,  0.08152859, ...,  0.04899672,\n",
       "         -0.06285532, -0.1115238 ],\n",
       "        [-0.07333675, -0.04321398,  0.07058871, ...,  0.04996576,\n",
       "         -0.10683268,  0.01793701],\n",
       "        ...,\n",
       "        [ 0.0366157 ,  0.11417486,  0.09402008, ..., -0.0904429 ,\n",
       "         -0.04173205,  0.10580213],\n",
       "        [-0.06769142,  0.05905014, -0.02266421, ..., -0.05005553,\n",
       "         -0.06818965,  0.03110234],\n",
       "        [ 0.04074491,  0.07985178,  0.12306023, ...,  0.00640144,\n",
       "         -0.07735749,  0.10592465]], dtype=float32),\n",
       " array([ 0.01264497,  0.        ,  0.00613535,  0.        ,  0.01870323,\n",
       "        -0.00638995, -0.01220101,  0.01602161,  0.        , -0.0035617 ],\n",
       "       dtype=float32),\n",
       " array([[ 0.32774705,  0.1360065 ,  0.12658864, -0.2245315 ,  0.02407579,\n",
       "         -0.4137968 ,  0.00840671, -0.32959276,  0.37902254,  0.09201939],\n",
       "        [ 0.19657296, -0.19044176, -0.45979258,  0.22773474,  0.34996778,\n",
       "          0.49039376, -0.11055854,  0.415774  ,  0.46002364, -0.39895314],\n",
       "        [ 0.47594562,  0.3576989 , -0.27898264,  0.24641478, -0.11810854,\n",
       "         -0.08997817, -0.39285222,  0.00848776, -0.4335943 ,  0.23973976],\n",
       "        [ 0.08522147, -0.46045008,  0.51219296, -0.03292036,  0.2374922 ,\n",
       "         -0.2037682 , -0.00391656, -0.51676846,  0.04617786, -0.4240946 ],\n",
       "        [-0.08442318,  0.5209976 , -0.50951713,  0.28533912, -0.09103562,\n",
       "          0.17975861, -0.42348057, -0.03162748,  0.15079224,  0.37590528],\n",
       "        [ 0.29116854,  0.02028061,  0.28541976,  0.21160066,  0.51888436,\n",
       "          0.2076629 ,  0.34932256,  0.53286564, -0.12616661,  0.5125922 ],\n",
       "        [ 0.14778593, -0.32210854, -0.51217043,  0.2680409 , -0.45627704,\n",
       "         -0.17553441,  0.3156804 ,  0.5589505 , -0.378236  , -0.15843673],\n",
       "        [-0.500389  ,  0.49888486,  0.46808332,  0.02391724,  0.22287901,\n",
       "          0.22456707,  0.04868002,  0.20557036, -0.51312673,  0.5572156 ],\n",
       "        [ 0.45945597,  0.19733101,  0.11253631, -0.43702573, -0.09192702,\n",
       "          0.43642336, -0.33800638, -0.48699063, -0.06122309,  0.42816746],\n",
       "        [-0.36811212,  0.10562864, -0.44934583,  0.20116043, -0.3474952 ,\n",
       "          0.07611621,  0.10975902,  0.29650488, -0.3930902 , -0.03682977]],\n",
       "       dtype=float32),\n",
       " array([-0.01129609,  0.0107806 ,  0.00960055,  0.01046941, -0.01452382,\n",
       "        -0.00337202,  0.00134577, -0.0105069 ,  0.        ,  0.01460001],\n",
       "       dtype=float32),\n",
       " array([[-0.3460477 ],\n",
       "        [ 0.41926035],\n",
       "        [ 0.23025933],\n",
       "        [ 0.03858984],\n",
       "        [-0.4711841 ],\n",
       "        [-0.5348796 ],\n",
       "        [ 0.58663434],\n",
       "        [-0.43052444],\n",
       "        [-0.23944312],\n",
       "        [ 0.11489856]], dtype=float32),\n",
       " array([0.0107341], dtype=float32)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO 4: Evaluate the model, calculating the metrics. -- DONE\n",
    "\n",
    "Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "          already compiled with the metrics.\n",
    "\"\"\"\n",
    "# performance = model.evaluate(transform_input(X_test), y_test)\n",
    "\n",
    "\"\"\"\n",
    "Option 2: Use the model.predict() method and calculate the metrics using\n",
    "          sklearn. We recommend this, because you can store\n",
    "          the predictions if you need more analysis later.\n",
    "          Also, if you calculate the metrics on a notebook,\n",
    "          then you can compare multiple classifiers.\n",
    "\"\"\"\n",
    "predictions = model.predict(y_train)\n",
    "accuracy = accuracy_score(y_test_original, predictions)\n",
    "f1_score = f1_score(y_test_original, predictions)\n",
    "\n",
    "print('accuracy in test:', accuracy)\n",
    "print('f1_score in test:', f1_score)\n",
    "\n",
    "# TODO 5: Save the results.\n",
    "# ...\n",
    "\n",
    "# One way to store the predictions:\n",
    "results = pandas.DataFrame(y_test_original, columns=['true_label'])\n",
    "results.loc[:, 'predicted'] = predictions\n",
    "results.to_csv('predictions_{}.csv'.format(args.experiment_name),\n",
    "               index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
